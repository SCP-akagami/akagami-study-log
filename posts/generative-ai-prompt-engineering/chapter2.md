---
title: '生成AIのプロンプトエンジニアリング-2章テキスト生成のための大規模言語モデル入門'
date: '2025-07-17'
tags: ['生成AIのプロンプトエンジニアリング', '大規模言語モデル', '自然言語処理']
---

# 生成AIのプロンプトエンジニアリング-2章テキスト生成のための大規模言語モデル入門

- トークン化にはいくつかの方法がある
  - バイト対符号化（Byte-Pair Encoding：BPE）：学習データに含まれる単語やフレーズが一般的なものではなかった場合であっても、認識し生成することが可能
  - WordPiece
  - SentencePiece

- ベクトル表現：言語の数値的性質
  - `w -> v = [v1, v2, ..., vn]`
  - 単語の埋め込み（Word Embedding）= 単語ベクトルの作成
  - トークン化と数値化の後に行われる
  - モデルはこれらの関係性を学習するように設計されており、意味が類似する単語が高次元空間で距離が近く、似た方向となるようにマッピングされる

- トランスフォーマー：文脈の関係性の調整
  - `vi' = Transformer(v1, v2, ..., vm)`
  - 文中の単語の単語ベクトルを扱い、それらの関係を、構造的にも意味的にも理解
  - BERT、GPTなどの多くのアーキテクチャが存在
  - セルフアテンション（Self-attnsion）のメカニズムにより、単語の微妙な文脈的意味を理解することが可能

- 確率的テキスト生成：決定メカニズム
  - `wnext = argmax P(w | w1, w2, ..., wm)`
  - 現在注目している単語の後に続く可能性のある単語セットのうち、どの単語がどのくらいの確率で後ろに続きやすいかを計算し、最も確率の高いものを選ぶ

